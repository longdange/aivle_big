# app.py
import streamlit as st
import torch
import clip
from PIL import Image
import pandas as pd
import os

# CLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# ë‹‰ë„¤ì„ ë° ì„±í–¥ ì½”ë“œ ë§¤í•‘ ì˜ˆì‹œ
nickname_map = {
    "WTIL": "ì—„ë§ˆ ê»Œë”±ì§€ ê²ìŸì´í˜•",
    "WTIA": "ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ ê´€ì°°í˜•",
    "WNIA": "ì„ ê¸‹ëŠ” ì™¸í†¨ì´ ì•¼ìƒê²¬í˜•",
    "WNIL": "íŒ¨ë‹‰ì— ë¹ ì§„ ê·¹ì†Œì‹¬í˜•",
    "WTEL": "ì´ˆë©´ì—” ì‹ ì¤‘, êµ¬ë©´ì—” ì¹œêµ¬",
    "WTEA": "í—ˆì„¸ ë¶€ë¦¬ëŠ” í˜¸ê¸°ì‹¬ìŸì´",
    "WNEA": "ë™ë„¤ ëŒ€ì¥ ì¼ì§„í˜•",
    "WNEL": "ê¹Œì¹ í•œ ì§€í‚¬ ì•¤ í•˜ì´ë“œí˜•",
    "CTEL": "ì‹ ì´ ë‚´ë¦° ë°˜ë ¤íŠ¹í™”í˜•",
    "CTEA": "ì¸ê°„ ì‚¬íšŒ ì ì‘ ë§Œë ™í˜•",
    "CNEA": "ë˜¥ê¼¬ë°œë„ í•µì¸ì‹¸í˜•",
    "CNEL": "ê³±ê²Œ ìë€ ë§‰ë‚´ë‘¥ì´í˜•",
    "CTIA": "ê°€ì¡± ë¹¼ê³¤ ë‹¤ ì‹«ì–´í˜•",
    "CTIL": "ëª¨ë²”ê²¬ê³„ì˜ ì—„ì¹œì•„í˜•",
    "CNIA": "ì£¼ì¸ì— ê´€ì‹¬ì—†ëŠ” ë‚˜í˜¼ì ì‚°ë‹¤í˜•",
    "CNIL": "ì¹˜ê³  ë¹ ì§€ëŠ” ë°€ë‹¹ ì²œì¬í˜•",
}

# ì„±í–¥ ì¶• ì •ì˜ [(ì™¼ìª½ ì½”ë“œ, ì™¼ìª½ ë¬¸ì¥), (ì˜¤ë¥¸ìª½ ì½”ë“œ, ì˜¤ë¥¸ìª½ ë¬¸ì¥)]
axis_prompts = [
    ("C", "ì´ ê°•ì•„ì§€ëŠ” ê°ì •ì ìœ¼ë¡œ êµë¥˜í•˜ê³  ì‹ ì²´ ì ‘ì´‰ì„ ì¢‹ì•„í•©ë‹ˆë‹¤."),
    ("W", "ì´ ê°•ì•„ì§€ëŠ” ë°˜ì‚¬ì ìœ¼ë¡œ ë³¸ëŠ¥ì ìœ¼ë¡œ í–‰ë™í•©ë‹ˆë‹¤."),
    ("T", "ì´ ê°•ì•„ì§€ëŠ” ì‹ ë¢°ì™€ ì•ˆì •ê°ì„ ë³´ì…ë‹ˆë‹¤."),
    ("N", "ì´ ê°•ì•„ì§€ëŠ” ë…ë¦½ì ìœ¼ë¡œ í–‰ë™í•˜ê³  í•„ìš”í•  ë•Œë§Œ êµë¥˜í•©ë‹ˆë‹¤."),
    ("E", "ì´ ê°•ì•„ì§€ëŠ” ì‚¬ëŒê³¼ ì ê·¹ì ìœ¼ë¡œ êµë¥˜í•©ë‹ˆë‹¤."),
    ("I", "ì´ ê°•ì•„ì§€ëŠ” í˜¼ì ìˆëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤."),
    ("A", "ì´ ê°•ì•„ì§€ëŠ” ìƒˆë¡œìš´ í™˜ê²½ì— í˜¸ê¸°ì‹¬ì´ ë§ê³  í™œë°œí•˜ê²Œ ì›€ì§ì…ë‹ˆë‹¤."),
    ("L", "ì´ ê°•ì•„ì§€ëŠ” ë‚¯ì„  í™˜ê²½ì— ì ì‘í•˜ì§€ ì•Šê³  ìµìˆ™í•œ ê³µê°„ì„ ì„ í˜¸í•©ë‹ˆë‹¤.")
]

# í”„ë¡¬í”„íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì„ë² ë”©
@st.cache_resource
def encode_texts():
    result = []
    for i in range(0, len(axis_prompts), 2):
        left_code, left_prompt = axis_prompts[i]
        right_code, right_prompt = axis_prompts[i+1]
        texts = clip.tokenize([left_prompt, right_prompt]).to(device)
        with torch.no_grad():
            feats = model.encode_text(texts).float()
            feats /= feats.norm(dim=-1, keepdim=True)
        result.append((left_code, feats[0].unsqueeze(0), right_code, feats[1].unsqueeze(0)))
    return result

axis_text_features = encode_texts()

# ì´ë¯¸ì§€ ì¸ì½”ë”© í•¨ìˆ˜
def encode_image(img):
    image = preprocess(img).unsqueeze(0).to(device)
    with torch.no_grad():
        img_feat = model.encode_image(image).float()
        img_feat /= img_feat.norm(dim=-1, keepdim=True)
    return img_feat

# CLIP ë§¤ì¹­ í•¨ìˆ˜
def predict_code(image_feat):
    code_letters = []
    for left_code, left_feat, right_code, right_feat in axis_text_features:
        left_sim = (image_feat @ left_feat.T).item()
        right_sim = (image_feat @ right_feat.T).item()
        code_letters.append(left_code if left_sim > right_sim else right_code)
    return "".join(code_letters)

# Streamlit ì•± UI
st.title("ğŸ¶ CLIP ê¸°ë°˜ ë™ë¬¼ ì„±í–¥(DBTI) ì˜ˆì¸¡ê¸°")

uploaded_files = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png"], accept_multiple_files=True)

if uploaded_files:
    results = []
    for uploaded in uploaded_files:
        img = Image.open(uploaded).convert("RGB")
        feat = encode_image(img)
        code = predict_code(feat)
        nickname = nickname_map.get(code, "ì•Œ ìˆ˜ ì—†ìŒ")

        st.image(img, width=200)
        st.markdown(f"**DBTI ì½”ë“œ**: `{code}`")
        st.markdown(f"**ì„±í–¥ ì„¤ëª…**: {nickname}")
        results.append({"íŒŒì¼ëª…": uploaded.name, "DBTI": code, "ë‹‰ë„¤ì„": nickname})

    df = pd.DataFrame(results)
    st.dataframe(df)

    # CSV ë‹¤ìš´ë¡œë“œ
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", csv, "dbti_result.csv", "text/csv")
